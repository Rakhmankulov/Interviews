# Вопросы к SRE/DevOps на интервью
- [Полезные ссылки](#полезные-ссылки)
- [Мои вопросы к интервьюеру](#мои-вопросы-к-интервьюеру)
- [DevOps практики](#devops-практики)
  - [Этапы CI/CD](#этапы-cicd)
- [Контейнеризация](#контейнеризация)
- [Что такое Kata Container](#что-такое-kata-container)
- [Что такое copy on write](#что-такое-copy-on-write)
- [12 факторов приложения](#12-факторов-приложения)
- [Docker](#docker)
  - [Что такое Docker и какие инструменты Linux лежат в его основе? Для чего он используется?](#что-такое-docker-и-какие-инструменты-linux-лежат-в-его-основе-для-чего-он-используется)
  - [Docker Building best practises](#docker-building-best-practises)
  - [Docker полный путь старта контейнера](#docker-полный-путь-старта-контейнера)
  - [Docker multi-stage builds](#docker-multi-stage-builds)
- [Kubernetes](#kubernetes)
  - [Компоненты Kubernetes](#компоненты-kubernetes)
  - [Компоненты контура управления Kubernetes](#компоненты-контура-управления-kubernetes)
  - [Компоненты worker-ноды Kubernetes](#компоненты-worker-ноды-kubernetes)
  - [Kubectl apply manifest - что происходит после этой команды (коротко)?](#kubectl-apply-manifest---%D1%87%D1%82%D0%BE-%D0%BF%D1%80%D0%BE%D0%B8%D1%81%D1%85%D0%BE%D0%B4%D0%B8%D1%82-%D0%BF%D0%BE%D1%81%D0%BB%D0%B5-%D1%8D%D1%82%D0%BE%D0%B9-%D0%BA%D0%BE%D0%BC%D0%B0%D0%BD%D0%B4%D1%8B-%D0%BA%D0%BE%D1%80%D0%BE%D1%82%D0%BA%D0%BE)
  - [Kubectl apply manifest - что происходит после этой команды (развернуто)?](#kubectl-apply-manifest---%D1%87%D1%82%D0%BE-%D0%BF%D1%80%D0%BE%D0%B8%D1%81%D1%85%D0%BE%D0%B4%D0%B8%D1%82-%D0%BF%D0%BE%D1%81%D0%BB%D0%B5-%D1%8D%D1%82%D0%BE%D0%B9-%D0%BA%D0%BE%D0%BC%D0%B0%D0%BD%D0%B4%D1%8B-%D1%80%D0%B0%D0%B7%D0%B2%D0%B5%D1%80%D0%BD%D1%83%D1%82%D0%BE)
  - [Как устроена сеть в Kubernetes](#как-устроена-сеть-в-kubernetes)
  - [Что такое Network Policies в Kubernetes](#что-такое-network-policies-в-kubernetes)
  - [Общие вопросы по Kubernetes](#общие-вопросы-по-kubernetes)
    - [Сущности Kubernetes (StatefulSets/Daemonset/Operator)](#сущности-kubernetes-statefulsetsdaemonsetoperator)
    - [Как поды назначаются на ноды](#как-поды-назначаются-на-ноды)
    - [Какие бывают Пробы](#какие-бывают-пробы)
    - [Как прокидывается файл из инит контейнера](#как-прокидывается-файл-из-инит-контейнера)
    - [Как выбирается мастер в Kubernetes](#как-выбирается-мастер-в-kubernetes)
    - [По какому протоколу работает API Kubernetes](#по-какому-протоколу-работает-api-kubernetes)
- [Linux](#linux)
  - [Команды дебага Linux](#команды-дебага-linux)
  - [Методы мониторинга](#методы-мониторинга)
  - [Общие вопросы с собесов](#общие-вопросы-с-собесов)
  - [Процессы и треды](#процессы-и-треды-шаринг-ресурсов)
  - [Шифрование](#шифрование)
  - [Где Prometheus хранит данные?](#где-prometheus-хранит-данные)
- [Ansible](#ansible)
- [Terraform](#terraform)
  - [Отличие Ansible и Terraform](#отличие-ansible-и-terraform)
  - [Основные возможности Terraform](#основные-возможности-terraform)
  - [Базовые команды Terraform](#базовые-команды-terraform)
  - [Важные концепции](#важные-концепции)
  - [Что такое ресурс в Terraform](#что-такое-ресурс-в-terraform)
  - [Как реализовать принцип Don't Repeat Yourself](#как-реализовать-принцип-dont-repeat-yourself-при-описании-инфраструктуры-с-помощью-iac-инструментов-terraform-ansible-helm)
- [Базы данных](#базы-данных)
  - [Типы баз данных](#типы-баз-данных)
  - [OLTP-базы данных](#oltp-базы-данных)
  - [Что такое Redis](#что-такое-redis)
- [Свистелки (Удобные инструменты DevOps)](#свистелки)
- [Менеджерские свистелки](#менеджерские-свистелки)
- [Вопросы без ответа](#вопросы-без-ответа)

## Полезные ссылки
https://github.com/Tinkoff/career/blob/main/interview/README.md - IT собеседование в Тинькофф  
https://learnk8s.io/production-best-practices - Kubernetes best practice  
https://github.com/Swfuse/devops-interview/blob/main/interview.md - IT вопросы на собесе  
https://www.opennet.ru/base/net/tcpdump_explore.txt.html - tcpdump  
https://habr.com/ru/company/alexhost/blog/531170/ - tcpdump  

## Мои вопросы к интервьюеру
- Эта позиция, на которую вы меня собеседуете - новая, или я заменю старого сотрудника? Чем не подошел старый сотрудник?  
- В какую команду вы меня берете и на какой проект?  
- Какие задачи решать и какую функцию я буду выполнять?  
- Как у вас построен процесс повышения грейдов и пересмотра зарплат?  
- На какой срок рассчитан этот проект?  
- Какие карьерные перспективы могут меня ждать в вашей компании?  

# Ответы на вопросы

---

## DevOps практики  
### Этапы CI/CD  
  `CI` - Непрерывная интеграция:  
    1. План  
    2. Код  
    3. Сборка  
    4. Тест  
  `CD` - Непрерывная доставка:  
    1. Тест  
    2. Релиз  
    3. Развертывание  
    4. Поддержка и мониторинг  
    Картинка:  
    ![DevOps Practics](https://github.com/Rakhmankulov/Interviews/blob/main/images/DevOpsPractics.jpeg)

---

## Контейнеризация
`Контейнеры` = Linux namespaces + crgoups + chroot + capabilities (Seccomp) 

  - `Namespaces` изолируют различные ресурсы между процессами, чтобы они не "видели" друг друга  
    [Механизмы контейнеризации: namespaces](https://selectel.ru/blog/mexanizmy-kontejnerizacii-namespaces/)    
    
    Namespaces состоят из:  
      | Тип      | Что изолирует |
      |-|-|
      | PID      | PID процессов |
      | NETWORK  | Сетевые устройства, сетки, порты и т.п. |
      | USER ID  | Пользовательские идентификаторы (UID/GID) |
      | MOUNT    | Точки монтирования (контейнер видит только свою файловую структуру) |
      | UTC      | Изоляция имени хоста и доменного имени |
      | IPC      | Изоляция систем межпроцессного взаимодействия (например, разделяемая память) |  
  
  - `Сgroups` (Control Groups) ограничивают, учитывают и изолируют использование ресурсов (CPU/Memory/Disk/Network)  
      | Ресурс   | Пример использования |
      |-|-|
      | CPU      | Ограничение использования процессорного времени |
      | Memory   | Задание лимитов на использование памяти |
      | Disk I/O | Контроль скорости чтения/записи на диск |
      | Network  | Ограничение пропускной способности сети |

      Cgroups предотвращают ситуацию, когда один контейнер потребляет все ресурсы хоста. Вот как они это делают:  

  - `Ограничение ресурсов` - можно определить сколько CPU или памяти будет использовать конкретный процесс  
  - `Приоритизация` – cgroups позволяют определить какой процесс в рамках одной `cgroup` более приоритетен для использования ресурсов  
  - `Отчетность` - лимиты ресурсов мониторятся и сообщаются на уровне `cgroup`  
  - `Управление` - можно изменить статус (frozen, stopped, restarted) всех процессов `cgroup` одной командой  
  - `Chroot` — операция изменения корневого каталога диска для запущенного процесса и его дочерних процессов. Программа, запущенная в таком окружении, не может получить доступ к файлам вне нового корневого каталога. Это измененное окружение называется `chroot jail`.  
  - `Seccomp` (сокращение от secure computing) — механизм ядра Linuх, позволяющий процессам определять системные вызовы, которые они будут использовать. Если злоумышленник получит возможность выполнить произвольный код, seccomp не даст ему использовать системные вызовы, которые не были заранее объявлены.  

**Итоговая структура контейнера:**  
1. **Файловая система** (основана на UnionFS, включает слои образов).
1. **Процессы** (изолированы с помощью namespaces).
1. **Ресурсы** (управляются через cgroups).
1. **Сеть** (виртуальная, изолированная через net namespace).
1. **Конфигурация** (переменные окружения, монтирование томов, настройки сети).

Контейнеры не включают ядро операционной системы — они используют ядро хоста, что делает их лёгкими и быстрыми по сравнению с виртуальными машинами.  

---

### Что такое Kata Container  
Это технология с открытым исходным кодом, которая создает легкие виртуальные машины, работающие и выглядящие как обычные контейнеры.

**Kata Containers** объединяют преимущества контейнеров и виртуальных машин (VM). Они обеспечивают более высокий уровень изоляции и безопасности для приложений, сохраняя при этом лёгкость и скорость работы контейнеров.
Если обычные контейнеры используют стандартные механизмы контейнеризации Linux, **у Kata Containers используются QEMU, KVM**.

Контейнер на стероидах. Больше для безопасности придумано.

---

### Что такое copy on write  
Принципы работы OverlayFS:  
  1. Чтение файлов:  
    - OverlayFS сначала ищет файл в верхнем слое.  
    - Если файл найден, он используется.  
    - Если файла нет в верхнем слое, он читается из нижнего слоя.  
  1. Запись файлов (Copy-on-Write):  
    - OverlayFS не изменяет файлы в нижнем слое, так как он read-only.  
    - При изменении файла из нижнего слоя:  
      - Файл копируется в верхний слой (это называется Copy-on-Write).  
      - Изменения применяются только к копии в верхнем слое.  
  1. Удаление файлов:  
    - OverlayFS не удаляет файлы из нижнего слоя.  
    - Вместо этого в верхнем слое создаётся запись, отмечающая файл как "удалённый" (Whiteout File).  
  1. Создание новых файлов:  
    - Новые файлы и каталоги создаются только в верхнем слое.  

---

### 12 факторов приложения

[12 факторов приложения](https://12factor.net/ru/) — это набор принципов для разработки современных веб-приложений, которые обеспечивают их масштабируемость, надежность и простоту развертывания. Вот краткое описание каждого из 12 факторов:
1. **Кодовая база**: Все приложения должны иметь одну кодовую базу, которая может быть развернута в нескольких средах (разработка, тестирование, продакшн).
2. **Зависимости**: Явно объявляйте и изолируйте зависимости приложения, чтобы обеспечить воспроизводимость среды.
3. **Конфигурация**: Храните конфигурацию в переменных окружения, а не в коде, чтобы упростить управление настройками для разных сред.
4. **Сервисы поддержки**: Внешние сервисы (например, базы данных, очереди сообщений) должны быть подключены через конфигурацию, а не жестко встроены в код.
5. **Сборка, релиз, запуск**: Четко разделяйте этапы сборки, релиза и запуска приложения для обеспечения стабильности и предсказуемости.
6. **Процессы**: Приложение должно быть многопроцессным, а не монолитным, чтобы обеспечить масштабируемость и устойчивость.
7. **Порты**: Приложение должно экспортировать сервисы через порты, а не через сокеты или другие механизмы.
8. **Параллельность**: Приложение должно быть способно масштабироваться горизонтально путем запуска нескольких экземпляров.
9. **Быстрые стартап и остановка**: Приложение должно быстро запускаться и корректно завершать работу, чтобы обеспечить надежность и удобство развертывания.
10. **Разделение разработки и продакшена**: Разработка и продакшен должны быть четко разделены, чтобы предотвратить нежелательные изменения в продакшене.
11. **Логи**: Приложение должно выводить логи в стандартный поток вывода, чтобы обеспечить централизованное управление логами.
12. **Административные процессы**: Административные задачи (например, миграции баз данных) должны выполняться в среде, идентичной продакшену, чтобы обеспечить надежность и предсказуемость.

---

## Docker
[50 вопросов по Docker, которые задают на собеседованиях, и ответы на них](https://habr.com/ru/companies/slurm/articles/528206/)  

---

### Что такое Docker и какие инструменты Linux лежат в его основе? Для чего он используется?

- Ответ
    
    **Docker** базируется на технологиях **namespaces**, **cgroups**, **capabilities**, **overlayfs**  
    * **namespaces** - обеспечивает изоляцию процессов. Например, можно айдишники процессов разместить в разных контейнерах.  
    * **cgroup** - ограничивают и управляют использованием ресурсов (CPU, память, дисковый ввод/вывод), которые контейнеры могут потреблять.  
    * **capabilites** - штука, которая позволяет дать некоторые **root** привелегии процессам или исполняемым файлам. Например, изменить UID процесса на 0, или дать возможность монтировать файловые системы.  
    * **overlay (overlayFS, overlay2-драйвер)** - файловая система, которая умеет работать "слоями". Не сохранять каждый раз новые файлы, а наслаивать один слой с конкретными изменениями конкретного файла на другой, тем самым экономя место на диске и время создания контейнера.  
    
    А вот **Docker** - это уже штука, которая всеми этими технологиями рулит. Да ещё и удобным для нас образом.
    

    Компоненты докера:  
    1. **Docker Daemon** — то самое Container Engine; запускает контейнеры.
    2. **Docker CLI** — утилита по управлению Docker Daemon.
    3. **Dockerfile** — инструкция по тому, как собирать образ.
    4. **Image** — образ, из которого раскатывается контейнер.
    5. **Container** - запущенный экземпляр образа — изолированная среда выполнения.
    6. **Docker registry** — хранилище образов.
    
    ![docker-basic-img](https://github.com/Rakhmankulov/Interviews/blob/main/images/Docker.png)
    
    На **DOCKER_HOST** работает **Docker daemon**, запускает контейнеры. Есть **Client** (Docker CLI), который передаёт команды: собери образ, скачай образ, запусти контейнер. **Docker daemon** ходит в **Registry** и выполняет их. Docker-client может обращаться и локально (к юникс-сокету), и по TCP с удалённого хоста.
    
    
    **Docker Daemon** (демон) — это серверная часть, она работает на хост-машине: скачивает образы и запускает из них контейнеры, создаёт сеть между контейнерами, собирает логи. Когда мы говорим «создай образ», этим тоже занимается демон.
    
    **Docker CLI** — клиентская часть Docker, консольная утилита для работы с демоном. Повторю, она может работать не только локально, но и по сети.

---

### Docker Building best practises
> [Официалная документация](https://docs.docker.com/build/building/best-practices/)
1. Использовать multi-stage билды
1. Создавать переиспользуемые стейджи
1. Выбор оптимальных базовых образов
1. Регулярная пересборка образов с обновленными зависимостями
1. Использовать --pull в команде `docker build` для загрузки более новой базовой версии:
    ```bash
    docker build --pull -t my-image:my-tag .
    ```
1. Использовать параметр --no-cache для чистой сборки:
    ```bash
     docker build --pull --no-cache -t my-image:my-tag .
    ```
1. Настроить исключения файлов с помощью `.dockerignore`
1. Создание эфемерных контейнеров (легко и быстро пересоздаваемых)

---

### Docker полный путь старта контейнера
1. Когда я выполняю `docker run hello-world`, Docker client отправляет запрос в Docker daemon по API.  
1. Daemon проверяет, есть ли образ локально, и если нет — скачивает его из registry.
1. Далее Docker daemon через containerd создаёт контейнер: настраивает namespaces, cgroups, файловую систему и через OCI runtime (runc) запускает процесс контейнера.
1. Stdout/stderr процесса передаются обратно через daemon в Docker client, который выводит результат в терминал.

---

### Docker multi-stage builds
При многоэтапной сборке вы используете **несколько операторов `FROM`** в вашем `Dockerfile`. Каждая инструкция `FROM` использует произвольный базовый образ и начинает новый этап сборки. Вы можете выборочно копировать артефакты с одного этапа на другой, оставляя только то, что вам необходимо в конечном образе. Пример как это работает:

Dockerfile:
```dockerfile
FROM golang:1.7.3
WORKDIR /go/src/github.com/alexellis/href-counter/
RUN go get -d -v golang.org/x/net/html  
COPY app.go .
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .

FROM alpine:latest  
RUN apk --no-cache add ca-certificates
WORKDIR /root/
COPY --from=0 /go/src/github.com/alexellis/href-counter/app . # тут вся фишка
CMD ["./app"]
```  

Теперь вам нужен только один `Dockerfile`. Более того, вам больше не нужен и отдельный скрипт сборки. Просто запустите сборку образа привычной командой.

```bash
docker build -t ivanovii/href-counter:latest .
```  

В итоге, вы получаете крошечный образ. Фишка в том, что вам не нужно извлекать промежуточные артефакты на вашу локальную систему.  

Как это работает? Вторая команда `FROM` начинает новый этап сборки с базового образа `alpine:latest`. Инструкция `COPY --from=0` копирует артефакты с предыдущего этапа сборки на текущий этап, благодаря чему необходимые для сборки Go SDK и любые промежуточные артефакты не сохраняются в конечном образе.  

---

## Kubernetes

### Компоненты Kubernetes 
![k8s](https://github.com/Rakhmankulov/Interviews/blob/main/images/Kuber.png)

---

### Компоненты контура управления Kubernetes  
  - `Kube-apiserver`:  
    - представляет API Kubernetes;  
    - умеет в горизонтальное масштабирование;  
    - на OpenAPI (OpenAPI - это cпецификация), на архитектуре REST.  
  - `etcd`:  
    - Распределённая и высоконадёжная БД в формате "ключ-значение", основное хранилище всех данных кластера в Kubernetes.  
  - `kube-scheduler`:  
    - отслеживает созданные поды без привязанного узла (ноды) и выбирает узел, на котором они должны работать;  
    - назначает поды согласно требованиям к ресурсам, ограничениям, связанным с аппаратными/программными политиками, принадлежности (affinity) и непринадлежности (anti-affinity) узлов/подов, местонахождению данных, предельным срокам отклика.  
  - `kube-controller-manager` (куча контроллеров в одном бинаре):  
    - `Node Controller`: уведомляет и реагирует на сбои узла;
    - `ReplicationController`: старая, изначальная технология, сейчас этой балалайки в `k8s` нет;
    - `ReplicaSets Controller`: поддерживает правильное количество подов для каждого объекта `ReplicaSets` в системе. У него нет `rolling-update`, как у предшественника. Вот по этому появился `Deployment`, он абстракция над `ReplicaSets`, добавляет функционал `rolling-update/rollback`. Костыли. Кругом костыли;  
    - `Endpoints Controller`: заполняет объект узлов (`Endpoints`), то есть связывает сервисы (`Services`) и поды (`Pods`);  
    - `Account & Token Controllers`: создают стандартные учетные записи и токены доступа API для новых пространств имен.  
> Контроллер - это асинхронный скрипт, который работает над согласованием текущего состояния системы `Kubernetes` с желаемым состоянием. Полезно думать о кубелете как о контроллере! Он запрашивает `Pods` из `kube-apiserver` каждые 20 секунд (это настраивается), фильтруя те, чье `NodeName` совпадает с именем узла, на котором запущен `kubelet`. Получив этот список, он обнаруживает новые добавления, сравнивая их с собственным внутренним кэшем, и начинает синхронизировать состояние, если имеются какие-либо расхождения.  

---

### Компоненты worker-ноды Kubernetes  
  - `kubelet`:  
    - следит за тем, чтобы контейнеры были запущены в поде;  
    - принимает набор `PodSpecs`, и гарантирует работоспособность и исправность определённых в них контейнеров;  
    - если `Pod` создается, он регистрирует некоторые метрики запуска, в `Prometheus` для отслеживания `Pod`;  
   
> У пода бывает несколько статусов: `Pending, Running, Succeeded, Failed and Unknown`. Когда `Pod` запускается впервые, `Kubelet` вызывает команду удаленной процедуры `RunPodSandbox (RPC)`. "Песочница" - это термин `CRI` для описания набора контейнеров, который в языке `Kubernetes`, как вы догадались, является `Pod`.  

  - `kube-proxy`:  
    - конфигурирует правила сети на узлах (iptables). При помощи них разрешаются сетевые подключения к вашим подам изнутри и снаружи кластера;  
> kube-proxy использует уровень фильтрации пакетов в операционной системы, если он доступен. В противном случае, kube-proxy сам обрабатывает передачу сетевого трафика.  

---

### Kubectl apply manifest - что происходит после этой команды (коротко)?  
  - валидация манифеста на стороне `kubectl` (client-side) и затем на `kube-apiserver` (server-side);  
  - `kubectl` идет в `kube-apiserver` на мастер-ноду, проходя при этом аутентификацию, авторизацию и admission control;  
  - `kube-apiserver` десериализует манифест и записывает объект в `etcd`;  
  - `controller-manager` смотрит на `kube-apiserver` через `informers/watches` и создает необходимые объекты (напр., ReplicaSet для Deployment);  
  - `kube-scheduler` получает уведомление о новых подах без привязки к ноде, подбирает подходящую ноду и устанавливает `nodeName` у пода;   
  - на воркер-ноде `kubelet` синхронизирует состояние подов через `CRI` (Container Runtime Interface) — `containerd`, `cri-o` или `docker`;  
  - `CRI` runtime создает контейнеры, `CNI` прокидывает сетевой интерфейс в pod, запускаются lifecycle hooks;
    
### Kubectl apply manifest - что происходит после этой команды (развернуто)?
[Источник](https://github.com/jamiehannaford/what-happens-when-k8s/blob/master/README.md)  

**1. Действия внутри kubectl (на стороне клиента)** 
  - Валидация и генераторы:  
    - `kubectl` проводит валидацию на стороне клиента, не отправляя никаких запросов  
    - `kubectl` формирует `HTTP` запрос, который будет отправлен на `kube-apiserver`
    - `kubectl` также выяснит, нужны ли какие-либо действия, например, запись команды (для роллоута или аудита), или же эта команда просто "--dry-run".  
  - API groups and version negotiation:
    - k8s использует REST архитектуру и протокол OpenAPI. Другими словами, API у кубера не монолитное и имеет много отдельных ресурсов. Собственно, выбирается нужный. Все ресурсы можно посмотреть командой `kubectl api-resources`  
    - у `k8s` версионированный API (v1, v1beta1, v1alpha1, apps/v1 и т.д.), kubectl выбирает подходящую версию для ресурса  
  - Аутентификация пользователя/клиента:  
    - Для аутентификации нужен `kubeconfig`  
    - Аутентифицироваться в k8s можно по сертификату, Bearer токену, имени пользователя/паролю, через OpenID токен   

**2. HTTP запрос отправляется на Kube-apiserver**  
`Kube-apiserver` - это главный интерфейс, через который клиенты и элементы контролплейна рулят нашим кластером.  
  - Аутентификация:  
    - `Kube-apiserver` должен убедиться, что запрашивающий является тем, за кого себя выдает. В зависимости от опциий запуска будет проверка что **HTTP-запрос закодирован ключом TLS**, подписанным корневым сертификатом CA, **или проверит токен**, или проверит что **учетные данные HTTP-запроса** соответствуют его собственному локальному состоянию.  
  - Авторизация:  
    - Например, через `RBAC` роль, настроенную админом.  
  - `Admission control`: (контроллеры допуска (последний бастион контроля перед сохранением объекта в `etcd`)):  
    > они бывают всякими, например, resource management, security, defaulting, and referential consistency examples of admission controllers resource management.    
    - `InitialResources` устанавливает ограничения ресурсов по умолчанию для ресурсов контейнера на основе прошлого использования;  
    - `LimitRanger` накладывает верхние границы на определенные ресурсы (не более 2 ГБ памяти, по умолчанию 512 МБ);  
    - `ResourceQuota` подсчитывает и запрещает количество объектов (pods, rc, балансировщики нагрузки сервисов) или общее количество потребляемых ресурсов (cpu, память, диск) в пространстве имен.

**3. После создания объекта, Kubernetes будет следить за существованием объекта**  
  - `ETCD`:  
    Здесь, наконец, `kube-apiserver` десериализует HTTP-запрос (inverse process of kubectl's generators) и записывает наши объекты в БД, такие как:  
      - Deployment
      - Replicaset
      - Pod
  > `Initializers` (инициализаторы): Ну на собесах фиг с ними

  - `Kube-controller-manager`:  
`Контроллеры` - это куча скриптов (участков кода) в одном бинарнике, которые асинхронно следят только за своей зоной ответственности.  
    - `Deployments controller` cоздает `ReplicaSet`, присоединяет к нему `label selector` и дает номер ревизии.  
    - `ReplicaSets controller` проверяет состояние нового `ReplicaSet`, если видит несоответствие между тем, что есть, и тем, что требуется, пытается наплодить `Pods`, принадлежащих `ReplicaSet`.  
      > При подъеме происходит магия `SlowStartInitialBatchSize`. То есть, при каждом успешном создании пода, новых подов в следующей итерации поднимается в 2 раза больше. Это сделано, чтобы не упороть `KubeApiserver`, если что-то пойдет не так.
      
  - `Informers` и `Watch`:    
    **Informers** — это высокоуровневый паттерн в Kubernetes для отслеживания изменений объектов. Компоненты (контроллеры, scheduler) подписываются через **Watch API** на изменения интересующих их объектов. Механизм **List-Watch** работает так: при старте выкачивается полное состояние ресурсов (List), а затем поддерживается актуальный локальный кэш через поток событий об изменениях (Watch). Это позволяет компонентам реагировать на события в режиме реального времени, потреблять меньше ресурсов и снижать нагрузку на Control Plane.  

  - `Scheduler`:  
    - Слушает события (через Watch) о подах, у которых не установлен `nodeName`, и выполняет scheduling в соответствии с запрошенными ресурсами, constraints и policies.
    - После выбора подходящей ноды устанавливает `nodeName` в spec пода через binding API.  
  - `Kubelet`:  
    - Это агент, запущенный на каждой воркер ноде кластера k8s и отвечает за управление жизненным циклом подов. Подписывается через Watch на поды, у которых `nodeName` соответствует этой ноде.
    - Получив информацию о новом поде, он обрабатывает всю логику перевода между абстракцией "Pod" и ее строительными блоками (контейнерами).  
    - Он также обрабатывает всю сопутствующую логику, связанную с:  
      - монтированием томов  
      - протоколированием контейнеров  
      - сборкой мусора и многими другими важными вещами  
      - передачей `podSpec` в `CRI`  
    - Регулярно сообщает статус подов обратно в `kube-apiserver`  
  - `CRI` (Docker, containerd (protocol buffers (it's like a faster JSON) and a gRPC API)):  
    - Создается `Sandbox` (песочница) включает в себя создание `pause` контейнера.  
    - `pause` контейнер служит в качестве родительского для всех остальных контейнеров в `Pod`, поскольку в нем размещается множество ресурсов уровня `Pod`, которые в конечном итоге будут использовать контейнеры рабочей нагрузки.  
    Эти `ресурсы` представляют собой пространства имен `Linux` (IPC, сеть, PID).  
  - `CNI`:  
    > bridge/ipvlan/macvlan/sr-iov
    
    тут прокидывается виртуальный линк в `Pod` хоста, `IP` цепляется за `pause` контейнер, чтоб не потеряться при рестартах контейнера из-за ошибок, например.  
  - `Inter-host networking`:  
    - `overlay networking` (Flannel, например)  
    Для этого обычно используется концепция оверлейной сети, которая представляет собой способ динамической синхронизации маршрутов между несколькими узлами. 
    Одним из популярных поставщиков оверлейных сетей является `Flannel`. Когда он установлен, его основной задачей является обеспечение сети `IPv4` третьего уровня между несколькими узлами в кластере. 
    `Flannel` не управляет тем, как контейнеры подключаются к узлу (это задача `CNI remember`), а скорее тем, как трафик передается между узлами. 
    Для этого он выбирает подсеть для узла и регистрирует ее в `etcd`. Затем он хранит локальное представление маршрутов кластера и инкапсулирует исходящие пакеты в UDP-датаграммы, гарантируя, что они дойдут до нужного узла.  
    > Для получения дополнительной информации ознакомьтесь с документацией CoreOS.  
  - `Container startup`: 
    - Pull the image  
    - Create the container via CRI  
    - Create network via CNI (всякие правила iptables)  
    - post-start lifecycle хуки  
    - Рединес пробы  
    - Контейнер шуршит  

---

### Как устроена сеть в Kubernetes  

Ладно, слушай. Сетка в Kubernetes это вообще интересная штука. Давай разберёмся, как тут всё устроено.

#### Самое главное правило
В Kubernetes все поды изначально друг с другом видят друг друга по IP-адресам. Это как если бы все твои контейнеры жили в одной большой локальной сети, независимо от того, на скольких нодах они крутятся. Звучит просто? Погоди, сейчас узнаешь, как это на самом деле работает.

#### Как поды получают IP адреса: сетевые пространства имён (netns)

Помнишь, мы говорили о `Pause контейнере`? Так вот, он держит `netns` — это сетевое пространство имён для пода.

Вот как это устроено:
- У каждого пода свой уникальный IP-адрес
- На каждой ноде есть **root netns** (корневое сетевое пространство). Главный сетевой интерфейс узла — eth0 — находится именно там
- У каждого пода есть свой **netns** с виртуальным Ethernet-интерфейсом. Этот интерфейс связан с root netns специальным "кабелем" — это называется `veth` (virtual ethernet)
- Представь себе это так: один конец кабеля `veth` торчит из root netns, другой конец — в netns пода. По этому кабелю передаются пакеты между подом и хостом

#### Трафик между подами на одной ноде (intra-node)

Давай посмотрим, что происходит, когда pod1 хочет отправить пакет pod2, и оба находятся на одной ноде:

1. Пакет выходит из pod1 через его виртуальный интерфейс eth0 и попадает в root netns узла через вещь под названием `veth` (вроде виртуального сетевого кабеля)
2. Пакет приходит в `cbr0` — это сетевой мост на ноде (container bridge). Это такой коммутатор в ядре ОС
3. `cbr0` говорит: "Хм, пакет для IP 10.x.x.x. У кого такой IP?" и отправляет ARP-запрос (это как узнать, кто сидит за этим IP)
4. Виртуальный кабель пода pod2 (`vethyyy`) отвечает: "Это я! Это мой IP!"
5. Мост узнаёт, через какой виртуальный кабель лежит путь до пода, и отправляет пакет туда
6. Пакет проходит через этот кабель и попадает в netns pod2

Просто, правда? Это работает на уровне L2 (линк-уровня).

#### Трафик между подами на разных нодах (inter-node)

А вот тут становится сложнее. Поды должны быть доступны **со всех узлов**, и каждому узлу нужен способ узнать, как до них добраться.

Вот как Kubernetes это решает:

**Каждому узлу назначается свой блок CIDR** — это диапазон IP-адресов. Например:
- Нода 1 получает 10.244.0.0/24
- Нода 2 получает 10.244.1.0/24

Так что поды на разных нодах никогда не конфликтуют по IP-адресам.

**Как трафик передаётся между нодами?**

Kubernetes не диктует, как это должно работать. Можно использовать несколько подходов:

1. **L3 маршрутизация** — просто добавляются маршруты в таблицу маршрутизации. Например: "Трафик для 10.244.1.0/24 идёт через 192.168.1.10 (IP узла 2)". Обычно облачные провайдеры используют именно этот способ
2. **Оверлейные сети** (вроде Flannel) — пакеты инкапсулируются. Например, пакет от пода на ноде 1 к поду на ноде 2 оборачивается в ещё один IP-пакет и отправляется на узел 2, а там распаковывается
3. **L2 подходы** (вроде Weave) — используется ARP между узлами

> Смысл такой: Kubernetes говорит "Поды должны видеть друг друга", а как это реализовать — дело CNI-плагина (Flannel, Calico, Weave и т.д.)

#### Services: как всё это упростить

Так, хорошо, но IP-адреса подов постоянно меняются — поды создаются и удаляются. Как приложениям обращаться друг к другу?

Для этого есть **Services** — это виртуальный IP (VIP), который не меняется и автоматически маршрутизирует трафик к нужным подам.

**Типы Services:**

- **`ClusterIP`** (по умолчанию): Сервис получает виртуальный IP, доступный только внутри кластера. Это как внутренний адрес склада, где он реально никому не нужен снаружи. `kube-proxy` на каждой ноде обновляет правила `iptables`, чтобы трафик на VIP перенаправлялся на реальные подов.

- **`NodePort`**: Сервис доступен на одном и том же порту на каждом узле кластера. Например, если ты настроил NodePort на порт 30000, то можешь обратиться на `192.168.1.10:30000` или `192.168.1.11:30000`, и трафик попадёт на поды сервиса. Под капотом Kubernetes всё равно создаёт ClusterIP, но дополнительно прокидывает трафик на узлах.

- **`LoadBalancer`**: Если ты в облаке (AWS, GCP и т.д.), этот тип просит облачный провайдер создать балансировщик нагрузки и направить трафик на твой сервис. Пользователь обращается на внешний IP балансера, и всё работает волшебно.

- **`ExternalName`**: Это просто alias для внешнего сервиса. Например, ты можешь сказать, что сервис `my-db` на самом деле это `my-db.example.com`. Kubernetes просто вернёт CNAME запись, никакого проксирования.

#### DNS в Kubernetes

Каждый сервис и под получают имя в внутренней DNS системе Kubernetes. Обычно это выглядит так:
- `my-service.default.svc.cluster.local` для сервиса `my-service` в namespace'е `default`
- Можешь просто писать `my-service`, и если под в том же namespace'е, система поймёт о ком речь

DNS-сервер в кластере (обычно CoreDNS) следит за всеми Services и обновляет DNS-записи. Это сделано для того, чтобы приложения могли обращаться друг к другу по имени, а не по IP.

#### kube-proxy: волшебник за кулисами

На каждой ноде работает `kube-proxy`. Его задача — следить за Services и обновлять правила маршрутизации в ядре ОС (обычно через `iptables`). Когда ты создаёшь Service, `kube-proxy` добавляет правило: "Если видишь трафик на VIP этого сервиса, отправь его на IP одного из его подов" (причём выбирает под по round-robin). Это работает очень быстро, потому что всё решается в ядре ОС.  
  
---

### Что такое Network Policies в Kubernetes  

**Network Policies** в Kubernetes — это объект, который определяет правила управления сетевым трафиком между подами (Pods) и сетевыми сущностями внутри кластера. Они используются для ограничения и контроля входящего (**ingress**) и исходящего (**egress**) трафика между подами на уровне сетевого взаимодействия.  

Сетевые политики позволяют улучшить безопасность, изолируя поды и управляя их сетевой доступностью. 

---

### **Основные задачи Network Policies:**
1. **Изоляция сетевого трафика:**  
   По умолчанию все поды могут взаимодействовать друг с другом без ограничений. Network Policies позволяют контролировать, какие поды могут обмениваться трафиком.

2. **Управление входящим трафиком (Ingress):**  
   Определяет, кто может отправлять сетевые запросы в под.

3. **Управление исходящим трафиком (Egress):**  
   Определяет, куда под может отправлять сетевые запросы.

---

### **Как работают Network Policies?**
- **Network Policies** работают на уровне **CNI (Container Network Interface)** плагинов, таких как:
  - Calico  
  - Weave  
  - Cilium  
  - Flannel (не поддерживает Network Policies).  

> **Важно:** Чтобы Network Policies работали, CNI-плагин должен поддерживать их реализацию.

- По умолчанию **поды открыты для всех**, пока не применена хотя бы одна Network Policy.

- **Правила политики** описываются на основе **меток (Labels)**, которые задают:
  - Какие поды участвуют в политике.
  - Какой трафик разрешён или запрещён.

---

### **Особенности работы:**
1. Если политика определена для **Ingress**, то **весь трафик блокируется**, кроме явно разрешённого.
2. Если политика определена для **Egress**, то весь исходящий трафик блокируется, кроме явно разрешённого.
3. Политики не имеют обратной силы:  
   - Если под не попадает в `podSelector`, политика на него **не распространяется**.
4. Network Policies **не блокируют** трафик, идущий через сервисы `NodePort` или `LoadBalancer`, а только межподовый трафик.

---

### **Преимущества использования Network Policies:**
1. **Безопасность:**  
   - Позволяют изолировать поды и ограничивать сетевые взаимодействия.
   - Защищают от несанкционированного доступа.

2. **Гибкость:**  
   - Позволяют задавать сложные правила маршрутизации трафика.

3. **Масштабируемость:**  
   - Использование меток (labels) упрощает управление политиками при большом количестве подов.

4. **Соответствие стандартам:**  
   - Network Policies помогают соответствовать требованиям безопасности, таким как PCI DSS или GDPR.

---

### **Ограничения Network Policies:**
1. **Только L3/L4:**  
   Network Policies работают на сетевом уровне (IP и порты), но не на уровне приложений (L7), например, HTTP.

2. **Зависимость от CNI-плагина:**  
   Network Policies не работают с плагинами, которые их не поддерживают (например, Flannel).

3. **Нет deny-правил:**  
   Kubernetes Network Policies определяют **только разрешающие (allow)** правила. Блокировка трафика происходит автоматически, если явно не разрешено.

---

### **Заключение:**
Network Policies в Kubernetes — это мощный инструмент для управления сетевым трафиком и изоляции подов. Они позволяют контролировать как входящие, так и исходящие соединения, обеспечивая безопасность и управляемость на сетевом уровне. Для их работы требуется CNI-плагин с поддержкой Network Policies, а правила задаются с использованием меток и диапазонов IP-адресов.

---

## Общие вопросы по Kubernetes  

### Сущности Kubernetes (StatefulSets/Daemonset/Operator)  
  - `StatefulSets`: Штука, описывающая где будут крутиться `Pods` с данными. [StatefulSets](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) 
  - `Daemonset`: Гарантирует, что все (или некоторые по .spec.template.spec.{nodeSelector или affinity}) узлы запускают копию `Pod` 
    - Примеры: 
      - node-exporter (Prometheus)
      - Datadog Agent
      - Zabbix agent
      - Telegraf
    - CNI / сетевые компоненты
      - Calico node
    - NVIDIA device plugin

  - `Operator`: (профессия) — группа профессий по управлению работой оборудования (установок) различного вида и назначения. Шутка. `Операторы` - это программные расширения `Kubernetes`, которые используют пользовательские ресурсы для управления приложениями и их компонентами.  
  К числу задач, которые можно автоматизировать с помощью оператора, относятся:
    - развертывание приложения по требованию;  
    - создание и восстановление резервных копий состояния приложения;  
    - обработка обновлений кода приложения вместе с сопутствующими изменениями, такими как схемы баз данных или дополнительные параметры конфигурации;  
    - публикация службы для приложений, которые не поддерживают API Kubernetes, чтобы обнаружить их;  
    - имитация сбоя всего кластера или его части для проверки его устойчивости; 
    - выбор лидера для распределенного приложения без внутреннего участника.  

### Как поды назначаются на ноды  
  Назначение подов на ноды происходит по одному из следующих методов:  
    - `nodeSelector`: выбор ноды по `node labels`. Kubernetes будет планировать размещение бода только на тех узлах, которые имеют все указанные вами метки.  
    - `Affinity and anti-affinity`: можно указать, что правило является мягким или предпочтительным, чтобы планировщик все равно запланировал под, даже если он не может найти подходящий узел, например.  
    - `nodeName`  
    - `Pod topology spread constraints`: ограничения на распространение топологии Подов

### Какие бывают Пробы  
  - `livenessProbe`: Показывает, запущен ли контейнер;  
  - `readinessProbe`: Указывает, готов ли контейнер отвечать на запросы;  
  - `startupProbe`: Указывает, запущено ли приложение внутри контейнера.  
  > Нужно выбирать или `livenessProbe` + `readinessProbe`, или `startupProbe`. Если с пробами что-то не так, `kubelet` смотрит в `restartPolicy`, а они бывают трех видов - `Always, OnFailure, and Never`

### Как прокидывается файл из инит контейнера  

Ладно, представь ситуацию: тебе нужно что-то подготовить перед тем, как запустить основное приложение. Например, сгенерировать конфиг, скачать данные или проверить что-то. Для этого есть **init-контейнеры** — они запускаются ДО основного контейнера и могут создавать/подготавливать файлы.

#### Порядок выполнения

Вот как это работает:

1. **Init-контейнер запускается первым** — выполняет свою задачу (например, создаёт файл `/data/config.conf`)
2. **Init-контейнер завершается** — его работа закончена, он умирает
3. **Основной контейнер запускается** — видит уже готовые файлы, которые создал init-контейнер
4. **Приложение работает** — использует файлы из общего тома

**Важно:** Если init-контейнер упадёт с ошибкой, основной контейнер вообще не запустится. Kubernetes перезапустит весь под.

#### Какие Volumes подходят

Для общего использования между init и основным контейнером подходят **emptyDir** (самый частый случай) и другие:

- **`emptyDir`** (самый частый) — пустая директория, которая существует только пока живёт под. Init создаёт там файлы, основной читает. Идеально!
  ```yaml
  volumes:
  - name: shared-data
    emptyDir: {}
  ```

- **`configMap`** — если нужно смонтировать конфиг из ConfigMap. Init может его читать и дополнять.

- **`downwardAPI`** — для метаинформации пода (имя, namespace, labels). Init может записать эту информацию в файл.

- **`secret`** — если init нужно прочитать секрет и обработать его.

- **`persistentVolumeClaim`** — если данные нужно сохранить между перезагрузками пода.

#### Когда и зачем использовать

**Когда использовать init-контейнеры:**

1. **Генерация конфигов** — например, создать nginx.conf на основе переменных окружения
2. **Загрузка данных** — скачать нужные файлы из S3 или другого хранилища
3. **Проверка зависимостей** — убедиться, что БД доступна, прежде чем запустить приложение
4. **Миграции БД** — запустить миграции перед стартом приложения
5. **Инициализация** — распаковать архив, создать нужные директории, установить разрешения

**Пример:**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  # Init-контейнер подготавливает данные
  initContainers:
  - name: init-config
    image: busybox
    command:
    - sh
    - -c
    - |
      echo "Generated config" > /data/config.txt
      echo "Server started at $(date)" >> /data/config.txt
    volumeMounts:
    - name: shared-data
      mountPath: /data
  
  # Основной контейнер читает готовые файлы
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - name: shared-data
      mountPath: /usr/share/nginx/html
  
  # Один том на всех
  volumes:
  - name: shared-data
    emptyDir: {}
```

В этом примере init-контейнер создаёт файлы в `/data`, а nginx их видит как готовые к использованию.

### Как выбирается мастер в Kubernetes?  
  Ответ: По протоколу RAFT  
    Основной механизм — Lease API:  
      * Kubernetes API предоставляет объект типа **Lease** (часть Coordination API).  
      * Кандидаты на лидерство периодически пытаются обновить объект **Lease** в **etcd**, который служит индикатором текущего лидера.  
      * Узел, который успешно обновляет **Lease**, становится лидером.  

### По какому протоколу работает API Kubernetes?  
  Ответ: OpenAPI (REST) HTTP/HTTPS  

### Два слова про `REST`  
    Обычно существует 2 поля объекта: `spec` и `status`.  
    В поле `spec` указывается требуемое состояние (описание характеристик, которые должны быть у объекта).  
    `status` описывает текущее состояние объекта  
    ой как понятно стало  

 `garbage collector` вычищает по полю `ownerReferences`  

### Восстановление etcd (disaster recovery):  
    - нужно выключить 2 из трех инстансов, на оставшемся можно подложить последний хороший бэкап (забэкапив текущие данные, если они покрашились) и запуститься с ключем `--force-new-cluster`  
    - [ETCD Disaster recovery](https://etcd.io/docs/v3.3/op-guide/recovery/)  
    > Сжатие + дефрагментация + правильный мониторинг — основа обслуживания вашего кластера etcd.  

---

## Linux
### Команды дебага Linux  
```bash
  lsof -p # поиск удаленных файлов, если процесс их еще держит
  numactl --hardware, -H # список номеров нод, разделенных запятыми, или диапазонов A-B, или всех.

  perf: # узнать сколько CPU использует каждая функция в программе
    CPU:
      perf top
      perf stat ls -e
      perf 
  /usr/bin/perf top -c 100

  strace # отслеживает системные вызовы
    strace -e trace=%network -p <PID> # смотреть сетeвые вызовы
    strace -f -p <PID> # посмотреть все дочерние процессы
    strace -e open -p <PID> # посмотреть все файлы, которые открывает процесс

  unshare # запуск программ в пространстве имен, не разделенном с родительским.
  ulimit -n # фиксит Too many open files
  opensnoop # следит за открытыми файлами
  dstat # сочетает в себе возможности iostat, vmstat, netstat и ifstat.
Докер:
  docker ps 

Ресурсы по ядру:
  syscalls https://man7.org/linux/man-pages/man2/syscalls.2.html
    The system call is the fundamental interface between an application and the Linux kernel.
Сетевые: 
  nslookup # DNS запросы
  tcpdump # можно проверить проходит ли пакет между хостами. Через wireshark можно посмотреть кто виноват, клиент или сервер. 
  tcpdump -A # посмотреть пакеты, если трафик не шифрованный
  tcpdump -ni any net 10.232.232.0/31 -AvX 
    -A # Печать каждого пакета (за вычетом заголовка уровня соединения) в формате ASCII. Удобно для захвата веб-страниц.
    -v # verbose
    -X # Печать данных каждого пакета в шестнадцатеричном формате и ASCII. 
  sudo ngrep -d any # сканить сетку
```
Чаще всего, программа работает медленно из-за:  
  - CPU time;  
  - Диска (много I/O операций);  
  - Ожидания медленного сервера.  

Mastering the mentioned tools: (strace, tcpdump, netstat, lsof, ngrep, etc)  

---

**IPIP over tcp или нет?**: Нет. TCP - L4, а IPIP и GRE-туннели — это туннели сетевого уровня (L3) модели OSI.
  IPIP и GRE-туннели — это туннели сетевого уровня (L3) модели OSI, при создании которых доступны IP-адреса обеих сторон. Они представляются в системе в виде интерфейсов GreX и IPIPX, и через них можно настраивать маршрутизацию (в том числе и default route) точно также, как и через любые другие интерфейсы.  
  Плюс ко всему для этих интерфейсов можно настроить уровень доступа security level — private, protected или public  
  IPIP (IP over IP) один из самых простых в настройке туннелей (инкапсулирует только unicast IPv4-трафик). Его можно настроить как на UNIX/Linux-системе, так и на различных маршрутизаторах (например, Cisco).  

**Как ООМ выбирает кого убивать?**: По приоритету. Приоритет в файле `/proc/$PID/oom_adj`. Сначала сдохнут свежие процессы пользователя с низким приоритетом, в последнюю очередь рутовые. Диапазон от -1000 (OOM Killer пройдет мимо) до 1000 (Процесс будет дохнуть в числе первых). ~~fsnotify~~

`UEFI`: новый `BIOS`  
`Stage boot loader`: синий экран загрузки, где можно задать `Single mod`. Представьте себе, и за знание таких терминов спрашивают. Еще бы спрашивали какой драйвер за вывод курсора на винде отвечает...   
`SIGHUP`: Сигнал HUP (повесить трубку) примерно такой же, как `SIGTERM` с точки зрения резкости, но у него есть особая роль, поскольку он автоматически отправляется приложениям, работающим в терминале, когда пользователь отключается от этого терминала. Это сигнал номер 1.  
**The signals SIGKILL and SIGSTOP cannot be caught or ignored**  

### Методы мониторинга  
  - `RED Method`:  
    - Requests  
    - Errors  
    - Duration  
  - `4 golden signals`:  
    - Задержка (Latency)  
    - Трафик (Traffic)  
    - Ошибки (Errors)  
    - Насыщенность (Saturation)  

## Общие вопросы с собесов  
### Процессы и треды (шаринг ресурсов)
  **Поток** — определенный способ выполнения процесса. Когда один поток изменяет ресурс процесса, это изменение сразу же становится видно другим потокам этого процесса.  
  
`IP` пода делится между всеми его контейнерами и является доступным (маршрутизируемым) для всех остальных подов.  

`DNS tcp или udp`: Как правило, считается, что DNS использует UDP port 53, но TCP port 53 также зарезервирован под использование для DNS. 
То есть, (маленький тупой) рекурсивный сервер должен сначала пробовать выполнять DNS-запрос с использованием UDP, но при высокой вероятности большого и усеченного ответа, либо при наличии открытой TCP-сессии к запрашиваемому серверу (по которому обрабатывается другой запрос или запрос другого клиента), не возбраняется использование TCP.
Подробнее [тут](https://www.securitylab.ru/blog/personal/aodugin/296669.php)  

**Чем отличается `git merge` от `git rebase`**  
  `merge` - оставляет историю (больше коммитов, после merge - новый коммит слияния в основной ветке, другая продолжает существовать и не зависит от основной)  
  `rebase` - переписывает историю (меньше коммитов, после rebase последний коммит перезаписывается и остаётся основным. Ветка, которая была родительской - удаляется)  

`Теорема cap`: эвристическое утверждение о том, что в любой реализации распределённых вычислений возможно обеспечить не более двух из трёх следующих свойств:  
  - согласованность данных (англ. consistency) — во всех вычислительных узлах в один момент времени данные не противоречат друг другу;  
  - доступность (англ. availability) — любой запрос к распределённой системе завершается откликом, однако без гарантии, что ответы всех узлов системы совпадают;  
  - устойчивость к разделению (англ. partition tolerance) — расщепление распределённой системы на несколько изолированных секций не приводит к некорректности отклика от каждой из секций.  

### Вопросы от Барсукова  
  У вас есть ноутбук с линуксом, например убунтой, на рабочем столе запущен терминал,
  в нем вы запускаете curl https://example.com/расскажите, что произойдет?

  Прострой отказоустойчивый сервис сокращения ссылок, например.  
  Отказоустойчивый, распределенный сервис DNS, например, на 5 зон  

### Шифрование 
  - Симметричное шифрование - один и тот же ключ используется и для кодирования, и для восстановления информации:  
    - Шифр Цезаря, RSA  
  - Асимметричное шифрование - данные шифруются открытым ключем, а расшифровываются приватным. И никакой магии. Примеры:  
    - TLS, SSH.  
    - mTLS добавляет возможность двусторонней аутентификации. Это означает, что не только клиент, но и сервер должны предоставить свидетельство своей подлинности при установлении защищенного соединения. Такое усиление аутентификации снижает риск атаки «человек посередине», когда злоумышленник пытается подставиться под сервер или клиента.  

---

### Где Prometheus хранит данные?

- Ответ:

  **Prometheus** хранит данные в своей **локальной Time Series Database (TSDB)** на диске.

    - **По умолчанию** — в директории `./data` (относительно рабочей папки Prometheus).
    - **В production** — обычно `/var/lib/prometheus` или `/var/lib/prometheus/data` (зависит от дистрибутива/установки).
    - Указывается флагом `--storage.tsdb.path=/path/to/data`.
    
    Структура внутри:  
    - WAL (write-ahead log) — для недавних данных и восстановления после краша  
    - Блоки (chunks_head, блоки по 2 часа → компактируются в большие) — основные данные time series  
    - Индексы, метаданные.
    
    Данные хранятся локально на файловой системе (рекомендуется локальный диск, не NFS для надёжности).  
    Для долгосрочного хранения → remote write в Thanos, Cortex, VictoriaMetrics и т.д. (локально держат только недавние данные, retention обычно 15 дней–несколько недель).

---

## Ansible

**Ansible** — это инструмент для **автоматизации настройки серверов**: устанавливать пакеты, править конфиги, запускать команды **без ручного SSH**.

> Проще: **Ansible — это “настроить много серверов одной командой”**.

---

## Структура директорий

Обычно проект выглядит так:

```
ansible/
├── inventory/        # Список серверов
│   └── hosts.yml
├── playbook.yml      # Что делать (основной сценарий)
├── roles/            # Готовые роли
│   └── nginx/
│       ├── tasks/    # Что выполнять
│       │   └── main.yml
│       ├── handlers/ # Перезапуск сервисов
│       │   └── main.yml
│       ├── templates/# Jinja2 шаблоны
│       ├── files/    # Файлы для копирования
│       └── vars/     # Переменные
```

### Коротко по файлам

* **inventory** — где сервера
* **playbook** — что делать
* **roles** — переиспользуемая логика

---

### Подлиннее по файлам

**Inventory** — файл конфигурации, где определяется информация о хосте. Хранится в папке /etc/ansible/hosts

**Плейбуки** - в плейбуках Ansible мы определяем, как применять политики, объявлять конфигурации, оркестрировать действия и запускать задачи на серверах — синхронно или асинхронно. Плейбук может включать один `play` или несколько.  

**Play** — компонент плейбука, который состоит из группы задач, выполняемых на определённых хостах. Каждый `play` должен указывать хост или группу хостов. Пример:  

```bash
-hosts: all # здесь мы указываем все хосты. 
```

**Задачи** `(task)` — это отдельные действия, которые выполняются плейбуками. Например:  
```bash
- name: Install Apache httpd # Определение задачи может содержать модули, 
например yum, git, service и copy.
```  

**Роли** в Ansible назначаются группе хостов и помогают организовать задачи по автоматизации. Мы можем создать несколько ролей и назначить их группе задач. Допустим, роль webserver можно использовать для установки Apache и Nginx на указанной группе серверов.  

**Обработчики** `(handler)` похожи на задачи, но выполняются только при вызове из события. Например, обработчик может запустить сервис httpd после того, как его установила задача. Обработчик вызывается директивой notify. Важно! Имя директивы notify и обработчика должны совпадать.  

**Шаблоны** Файлы шаблонов `(template)` основаны на шаблонизаторе `Python Jinja2` и имеют расширение `.j2`. В файл шаблона при желании можно поместить содержимое файла `index.html`, но эффективнее всего использовать в них переменные и факты.  

**Переменные** В плейбуки можно включать собственные переменные. Есть пять способов определить переменную:  

1. В play, в атрибуте vars_files:  
```bash
vars_files:
- "/path/to/var/file"
```
2. В <role>/vars/apache-install.yml
3. В командной строке:  
```bash
# ansible-playbook apache-install.yml -e "http-port=80"
```
4. В `play` через vars:  
```bash
vars: http_port: 80
```
5. В каталоге group_vars/  

**Приоритеты переменных** https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_variables.html#understanding-variable-precedence  

---

### Модули в Ansible

**Модули в Ansible** — это **готовые действия**, которые Ansible выполняет на серверах.

> Проще: **модуль = “что именно сделать”**
> (установить пакет, скопировать файл, перезапустить сервис).

---

## Примеры модулей

* `apt` / `dnf` — установить пакет
* `copy` — скопировать файл
* `template` — развернуть шаблон
* `service` / `systemd` — управлять сервисом
* `file` — создать файл/директорию
* `shell` / `command` — выполнить команду

---

## Пример в playbook

```yaml
- name: Install nginx
  apt:
    name: nginx
    state: present
```

Здесь `apt` — **модуль**.

---

## Важно понимать

* Модули **идемпотентны** (повторный запуск безопасен)
* Работают через SSH
* Не нужно писать скрипты
* Возвращают structured output (ok / changed / failed)

---

## Что НЕ является модулем

* playbook — ❌
* role — ❌
* inventory — ❌

---

## Terraform

### Отличие Ansible и Terraform 

**Terraform** - Предназначен для инфры.  
**Ansible** - Предназначен для приложений на инфре.  

  То есть, **Terraform** используется чтобы разворачивать инфраструктуру (виртуальные машины, сети, диски, и тп.).  
  **Ansible** предназначен для работы с существующими ресурсами, чтобы устанавливать и настраивать окружение (программы, операционные системы) на этих виртуалках.  

Нюансы:
* **Terraform** отслеживает состояние и делает инкрементные изменения, что особенно полезно при работе с облачной инфраструктурой.
* Ansible не управляет состоянием, и каждая задача считается независимой.

  * Нужно понимать что это инструменты. И в ряде случаев с помощью **Ansible** равзвернуть инфраструктуру всё таки можно. 
  * Как и для **Терраформа** написать провайдер, который будет делать что-то свое. Вопрос лишь в применимости инструментов.  

### Основные возможности Terraform:

* **Декларативный язык (HCL)** – вы описываете, **какой должна быть инфраструктура**, а Terraform сам выполняет необходимые шаги.
* **Поддержка множества облаков** – AWS, Google Cloud, Azure, Kubernetes и даже локальная инфраструктура.
* **Отслеживание состояния** – Terraform хранит текущее состояние ресурсов и понимает, что нужно изменить.
* **Масштабируемость** – можно управлять как небольшими, так и крупными кластерами.
* **Модули** – позволяют переиспользовать код и упрощать управление инфраструктурой.

---

### Базовые команды Terraform?

1. **Пишем код**
   Определяем ресурсы в файле `.tf`, например:

   ```hcl
   resource "aws_instance" "example" {
     ami           = "ami-123456"
     instance_type = "t2.micro"
   }
   ```
2. **Инициализируем проект**

   ```sh
   terraform init
   ```
3. **Просматриваем план изменений**

   ```sh
   terraform plan
   ```
4. **Применяем конфигурацию**

   ```sh
   terraform apply
   ```
5. **Удаляем инфраструктуру при необходимости**

   ```sh
   terraform destroy
   ```

### Важные концепции:

* **Providers** – плагины, которые позволяют работать с разными облачными сервисами.
* **State (состояние)** – Terraform хранит текущее состояние ресурсов в файле `terraform.tfstate`.
* **Modules** – повторно используемые блоки конфигурации.
* **Variables** – позволяют параметризовать конфигурацию.
* **Outputs** – используются для экспорта данных, например, IP-адресов.

---

### Что такое ресурс в Terraform

- Ответ:

  Это **основной строительный блок** кода, который описывает объект в инфраструктуре. Каждый ресурс имеет свои аргументы, свои параметры, свою конфигурацию. Ресурс имеет какой-то уникальный идентификатор (ID, ARN и тд).

  Виртуальная машина, сеть, диск, порт, балансировщик и тд.

---

### Как реализовать принцип Don't Repeat Yourself при описании инфраструктуры с помощью IaC-инструментов (Terraform, Ansible, Helm)?

- Ответ: 
  В Terraform — модули, for_each, переменные + locals.  
  В Ansible — роли, include_tasks, vars_files и Jinja-шаблоны.  
  В Helm — _helpers.tpl, values.yaml + subcharts.
  
  Главное — выносить повторяющуюся логику в переиспользуемые блоки и параметризовать через   переменные.

---

## Базы данных

---

### Типы баз данных  
  - Реляционные - SQL (мощная аналитика):   
    - PostgreSQL  
    - MySQL  
    - MariaDB  
    - Oracle  
    - SQLite  
  - NoSQL - документные, похожие на Json (Быстрый доступ к конкретной информации):  
    - MongoDB  
    - CouchDB  
  - NoSQL - key: value (Ключ: значение)  
    - Redis  
    - Memcached
    - ETCD  
  - NoSQL - Временные ряды:  
    - Prometheus  
    - TimescaleDB  
  - NewSQL:  
    - ...  
  - Графовые:  
    -  Dgraph
    -  Giraph

### OLTP-базы данных  
**OLTP (Online Transaction Processing) база данных** — это тип базы данных, предназначенный для обработки большого количества транзакций 
в реальном времени. Используется там, где требуется высокая скорость операций ввода, обновления, удаления и чтения данных, 
таких как системы управления заказами, банковские системы, розничная торговля, системы бронирования и другие.  

**Основные характеристики OLTP-баз данных**  
1. **Обработка транзакций в реальном времени:**  
  * OLTP-базы обрабатывают транзакции (например, покупка товара, перевод средств) быстро и с минимальной задержкой.  
1. **Частые операции чтения и записи**:  
  * Большинство операций связаны с короткими транзакциями, которые часто включают добавление, обновление или удаление данных.  
1. **Моделирование данных**:  
  * OLTP использует нормализованную структуру данных, чтобы минимизировать избыточность и обеспечивать согласованность данных.
1. **Поддержка ACID-операций**:  
  * OLTP-системы гарантируют:
    * **Атомарность (Atomicity)**: Все изменения, связанные с транзакцией, выполняются полностью или не выполняются вообще.
    * **Согласованность (Consistency)**: Данные всегда остаются в согласованном состоянии.
    * **Изолированность (Isolation)**: Одновременные транзакции не мешают друг другу.
    * **Долговечность (Durability)**: Завершённая транзакция остаётся сохранённой даже в случае сбоя системы.
1. **Поддержка большого числа пользователей**:  
  * OLTP-системы часто рассчитаны на поддержку тысяч или миллионов пользователей, которые выполняют транзакции одновременно.  

### Примеры баз данных, используемых для OLTP:  
1. Реляционные базы данных:  
  * MySQL
  * PostgreSQL
  * Oracle Database
  * Microsoft SQL Server
2. NoSQL базы данных (для специфичных сценариев):  
  * MongoDB
  * Cassandra
  * Redis (в качестве высокопроизводительного кэша)

---

### **Рекомендованная архитектура отказоустойчивой БД:**
1. Основная база данных с синхронной репликацией (Master-Slave).
2. Географически распределённые узлы для защиты от катастроф.
3. Автоматическое переключение (failover) на резервный сервер.
4. Резервное копирование с регулярной проверкой восстановления.
5. Инструменты мониторинга и оповещения для быстрого реагирования.

---

### **Итоговая архитектура отказоустойчивого кластера PostgreSQL**  
1. **Primary и минимум одна синхронная реплика.** Для синхронной реплики нужно включить WAL-журналы (Write-Ahead Logging - Все операции записываются в журнал транзакций перед их применением)  
2. **Автоматический failover через Patroni или Pgpool-II.** - Pgpool-II Позволяет распределять чтение на реплики.  
3. **VIP для прозрачного переключения клиентов.** - виртуальный IP, через Keepalived
4. **Мониторинг и управление через Prometheus/Grafana.**
5. **Резервные копии для защиты от потери данных.** через pg_basebackup или pg_dump

---

### Что такое Redis
Redis (Remote Dictionary Server) — это сверхбыстрая база данных с открытым исходным кодом, которая хранит данные в оперативной памяти и поддерживает множество типов данных. Она популярна как кэш, брокер сообщений или хранилище данных, требующих быстрого доступа.  

---

## Свистелки  
  `Argo cd` (continuous delivery): декларативный инструмент для непрерывной доставки в Kubernetes по модели `GitOps`  
  `Terragrunt` — это тонкая обертка (wrapper) для `Terraform`, которая решает его главные «боли» и помогает содержать код инфраструктуры в чистоте, особенно в больших проектах. Если Terraform — это кирпичи, то Terragrunt — это **умная система логистики**, которая следит, чтобы кирпичи лежали ровно и не дублировались.  
  `Istio Service Mesh`:
    - `Istio` идет как `Control Plane`, он состоит из трёх компонентов - `Pilot`, `Mixer` и `Citadel`;  
    - `Envoy` как `sidecar` у подов (Data Plane)  
      > Istio перехватывает весь сетевой трафик и применяет к нему набор правил, вставляя в каждый pod умный прокси в виде sidecar-контейнера. Прокси, которые активируют все возможности, образуют собой Data Plane, и они могут динамически настраиваться с помощью Control Plane.  
    > На моей практике `Nginx` сайта перенаправлял запросы к одной из нескольких ферм `Envoy`, которая роутила запрос непосредственно в нужный `kubernetes` кластер. 
    
  `ELK: (Elasticsearch, Logstash и Kibana)`: у нас ClickHouse, c kittenhouse - своей проксей + statsd  
  `sr-iov`: Single Root Input/Output Virtualization  
  `Calico`: сетевая штука, через `CRD` (Custom Resource Definition) поднимает `networkpolicy`, `net`  
  `Consul`: обеспечивает обнаружение сервисов, проверку работоспособности, балансировку нагрузки, граф сервисов, принудительное использование идентификационных данных с помощью TLS и управление конфигурацией распределенных сервисов. Вау.    
  `Deckhouse kubernetes` — это enterprise-платформа Kubernetes “под ключ” от компании Flant, ориентированная на эксплуатацию, безопасность и управляемые обновления.  
  `Mesos`: это централизованная отказоустойчивая система управления кластером. Она разработана для распределенных компьютерных сред c целью обеспечения изоляции ресурсов и удобного управления кластерами подчиненных узлов (mesos slaves). Это новый эффективный способ управления серверной инфраструктурой, но и, как любое техническое решение, не "серебряная пуля". Нормальные мальчики пользуют ванильный кубер :)  
  `Apache Kafka`:  
    - Событие или сообщение — данные, которые поступают из одного сервиса, хранятся на узлах Kafka и читаются другими сервисами. Сообщение состоит из:  
      - **Key** — опциональный ключ, нужен для распределения сообщений по кластеру.  
      - **Value** — массив байт, бизнес-данные.  
      - **Timestamp** — текущее системное время, устанавливается отправителем или кластером во время обработки.  
      - **Headers** — пользовательские атрибуты key-value, которые прикрепляют к сообщению.  

### Менеджерские свистелки
  `Nexus`: фреймворк - расширение классического скрама для крупных проектов с многокомандной разработкой.  
  `ITIL методология`: https://www.atlassian.com/ru/itsm/itil  
  `ITSM методология` Основная идея ITSM заключается в том, что ИТ нужно предоставлять как услугу.

---

### Вопросы без ответа 
  Понимание BGP: это протокол динамической маршрутизации  

  Dependencies  
  lvm  
  jagger  
  open-api  
  gravitee  
  vmware - виртуализация, не интересно в эпоху контейнеров. Хотя... Есть люди, которые поднимают `Kubernetes` в виртуалках, наверно, надо изучить эти кейсы. Здесь может быть ваш MR :)  

LeetCode - уровень easy https://leetcode.com/ - это для SRE в Rндекс и ТинькоFF. Кстати, они гоняют тупо по академическим знаниям, не спрашивая у любителей куберетиса ничего про кубернетис, отнимая тонны вашего времени на 4 этапа. Что сказать, Google Way...  

штурвал  
haiva  
ipv6  

git джит-компилятор  

steal  
dora  

Безопасность:  
  dlp системы  
  
  RHEL  
  Пайпы  
