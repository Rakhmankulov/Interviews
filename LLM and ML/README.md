# Вопросы к LLMOps/MLOps на интервью
- [Инструменты инфраструктуры LLM](#инструменты-инфраструктуры-llm)
  - [NVIDIA](#nvidia)
  - [NVIDIA GPU Operator](#nvidia-gpu-operator)
- [Inference](#inference)
  - [Ollama](#ollama)
  - [vLLM](#vllm)
  - [RAG](#retrieval-augmented-generation-rag)
## Разобрать следующие вопросы:
1. Базовые знания MLOps (модели, fine-tuning, quantization, caching).
1. Инструменты инфтраструктуры LLM
1. LiteLLM, Open WebUI и Langfuse
1. Что такое Tensor Processing Unit (TPU)?


---
## База

# Инструменты инфраструктуры LLM

### NVIDIA  

---
### NVIDIA GPU Operator  
[оф дока](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/index.html)  
Это инструмент для управления видеокартами NVIDIA внутри кластера Kubernetes. Разворачивается как Helm-Chart. 
Он сам обнаруживает новые узлы с GPU и подготавливает их к работе - устанавливает драйверы, настраивает библиотеки, ставит метки с информацией о картах на ноды и вводит их в эксплуатацию. А также, собирает метрики.

**Основные компоненты:**
1. **NVIDIA Driver**: Самый важный слой. Это программное обеспечение, которое объясняет операционной системе (и Kubernetes), как общаться с «железом» видеокарты.
2. **NVIDIA Container Toolkit**: Мостик между контейнерами и видеокартой. Позволяет приложениям внутри Docker-контейнеров «видеть» графический процессор и использовать его мощности.
3. **Kubernetes Device Plugin**: Регистрирует видеокарты как ресурсы в кластере. Благодаря ему Kubernetes понимает, что на узле есть, например, 4 свободные GPU, и может отправить туда ваши задачи.
4. **Node Feature Discovery (NFD)**: «Разведчик», который сканирует оборудование сервера и ставит на него метки (labels), сообщая оператору: «Эй, здесь есть видеокарта NVIDIA!».
5. **DCGM Exporter**: Сборщик метрик. Он вытягивает данные о температуре видеокарты, загрузке памяти и энергопотреблении, чтобы вы могли видеть красивые графики в Grafana.
6. **GFD (GPU Feature Discovery)**: Дополняет метки NFD более детальной информацией — например, сообщает конкретную модель видеокарты (A100, T4) и объем видеопамяти.
7. **MIG Manager**: Полезен для мощных карт (как A100 или H100). Позволяет «разрезать» одну физическую видеокарту на несколько независимых частей, чтобы их могли использовать разные пользователи одновременно.

---
### NVIDIA MIG Profiles (Multi-Instance GPU)
[оф дока](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/getting-started-with-mig.html) 
Позволяет нарезать мощные и дорогие видеокарты на нужные кусочки и раздавать их инференс моделям разного размера, тем самым плотно утилизируя дорогое железо.

Разберем кодировку профилей, например, 1g.5gb:  
* **Цифра перед «g»** (1g, 2g, 3g...): Это количество «вычислительных долек» (GPU Slices). Всего у A100 их 7. Если ты берешь 1g, ты забираешь 1/7 мощности. Если 7g — забираешь всё.
* **Цифра после точки** (5gb, 10gb, 20gb...): Это объем выделенной видеопамяти.

**Важные ограничения из документации:**
*   **Фиксированные сетки (Placements):** Нарезка GPU ограничена физической структурой чипа; нельзя создать произвольную комбинацию профилей, если она не вписывается в поддерживаемую сетку размещения.
*   **Отсутствие динамического изменения:** Для смены профиля необходимо остановить текущие процессы и перенастроить инстанс.
*   **Поддержка архитектур:** Работает только на картах серий Ampere (A100, A30) и Hopper (H100).
*   **Только вычисления:** MIG разработан для CUDA-задач и не поддерживает графические API (OpenGL, Vulkan).
*   **Изоляция:** Обеспечивает полную аппаратную изоляцию (QoS), гарантируя, что нагрузка в одном инстансе не влияет на производительность другого.
---
## Inference
**Inference** — это использование обученной модели для получения предсказаний или генерации ответа без изменения её весов.

Ниже — коротко и по делу, в формате, который удобно сразу положить в Markdown-доку.

---

## Inference runtime серверы

**Inference runtime сервер** — это сервис, который загружает ML/LLM-модель в память (CPU/GPU) и предоставляет API для выполнения инференса (generations, embeddings, chat и т.п.).
Он отвечает за:

* загрузку и хранение модели в памяти,
* эффективное использование GPU/CPU,
* батчинг и параллелизм запросов,
* управление контекстом и KV-cache,
* API (чаще всего OpenAI-compatible).

Используется как backend для приложений, RAG, агентов и внутренних сервисов.

---

## Ollama

**Ollama** [оф дока](https://docs.ollama.com/quickstart) — простой inference runtime, ориентированный на локальный запуск моделей.

### Ключевые особенности

* Запуск LLM «из коробки» одной командой
* Минимум настроек
* Поддержка CPU и GPU
* Свой формат моделей (`Modelfile`)
* HTTP API + CLI

### Плюсы

* Очень простой старт (dev / PoC)
* Отлично подходит для локальной разработки
* Минимальные требования к инфраструктуре

### Минусы

* Ограниченная масштабируемость
* Слабый контроль над GPU-ресурсами
* Не оптимизирован для высокой нагрузки
* Не лучший выбор для продакшена и multi-tenant сред

### Типовые use-cases

* Локальная разработка
* Демонстрации
* Тестирование моделей
* Небольшие внутренние сервисы

---

## vLLM

**vLLM** [оф дока](https://docs.vllm.ai/en/latest/getting_started/quickstart/) — высокопроизводительный inference runtime, заточенный под продакшен-нагрузки.

### Ключевые особенности

* Оптимизирован под GPU
* PagedAttention (эффективный KV-cache)
* Continuous batching (динамическая агрегация запросов)
* Высокий throughput и низкий latency
* OpenAI-совместимый API
* Отлично работает в Kubernetes

### Плюсы

* Максимально эффективное использование GPU
* Хорошо масштабируется
* Подходит для multi-tenant сценариев
* Фактический стандарт для LLM-инференса в k8s

### Минусы

* Более сложная настройка
* Требует понимания GPU и параметров модели
* Избыточен для простых локальных сценариев

### Типовые use-cases

* Продакшен-инференс LLM
* RAG-системы
* Внутренние AI-платформы
* Multi-user и high-load сервисы

---

## Краткое сравнение

| Характеристика    | Ollama         | vLLM               |
| ----------------- | -------------- | ------------------ |
| Цель              | Простота       | Производительность |
| Основной сценарий | Локально / Dev | Продакшен          |
| Масштабирование   | Ограниченное   | Отличное           |
| GPU эффективность | Средняя        | Очень высокая      |
| Kubernetes        | Не фокус       | First-class        |
| OpenAI API        | Частично       | Да                 |

---
### Retrieval-Augmented Generation (RAG)

**RAG (Retrieval-Augmented Generation)** — это технология, которая дает нейросети «внешнюю память» или «справочник», чтобы она перестала галлюцинировать и могла отвечать на основе свежих или закрытых данных.

Если простая LLM — это **студент на экзамене**, который полагается только на свою память, то RAG — это **тот же студент, которому разрешили пользоваться библиотекой**.

### Как это работает (3 простых шага):

1.  **Поиск (Retrieval):** Когда вы задаете вопрос, система идет не сразу в нейросеть, а в специальную базу данных (обычно векторную). Там она ищет куски текста, которые по смыслу похожи на ваш запрос.
2.  **Дополнение (Augmented):** Найденные куски текста (контекст) приклеиваются к вашему вопросу. Получается длинный промпт: *«Используя вот эту информацию: [найденный текст], ответь на вопрос: [ваш вопрос]»*.
3.  **Генерация (Generation):** Нейросеть читает предоставленный текст и выдает точный ответ, опираясь на факты из «справочника».

### Почему это круто:
*   **Актуальность:** Не нужно переобучать модель, чтобы она узнала о сегодняшних новостях — достаточно обновить базу документов.
*   **Отсутствие галлюцинаций:** Модель ограничена вашим текстом. Если в базе нет ответа, она может сказать «я не нашел информации», а не выдумывать.
*   **Безопасность:** Можно кормить нейросети корпоративные документы, не отправляя их на дообучение в OpenAI (или любой другой внешней LLM-ке).
*   **Пруфы:** В ответе RAG-системы часто можно увидеть ссылку на конкретный документ, из которого взят факт.

### Из чего состоит RAG-стек:
*   **Векторная база данных** (Qdrant, Pinecone, Milvus) — где лежат ваши знания.
*   **Эмбеддинги** (модели вроде `text-embedding-3`) — превращают текст в математические векторы для поиска.
*   **Оркестратор** (LangChain, LlamaIndex) — связывает поиск и генерацию в один пайплайн.